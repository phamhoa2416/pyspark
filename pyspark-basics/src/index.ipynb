{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Chocolate Sales').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "df_spark = spark.read.option(\"header\", \"true\").csv(\"../data/Chocolate Sales.csv\", inferSchema = True)\n",
    "\n",
    "# Another option to read dataset\n",
    "spark.read.csv(\"../data/Chocolate Sales.csv\", header=True, inferSchema=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the schema\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the column names\n",
    "df_spark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first 10 rows\n",
    "df_spark.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the Product column\n",
    "df_spark.select('Product').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select multiple columns\n",
    "df_spark.select(['Product', 'Country', 'Date']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the types of the columns\n",
    "df_spark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the summary of the dataset\n",
    "df_spark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column\n",
    "df_spark = df_spark.withColumn('New Column', df_spark['Boxes Shipped'] * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns\n",
    "df_spark = df_spark.drop('New Column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns\n",
    "df_spark.withColumnRenamed('Sales Person', 'Salesperson').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the missing values\n",
    "\n",
    "# how parameter\n",
    "df_spark.na.drop(how = 'any') # drop column if any missing value - default\n",
    "df_spark.na.drop(how = 'all') # drop column if all missing value\n",
    "\n",
    "# threshold parameter\n",
    "df_spark.na.drop(thresh = 2) # specify the minimum number of non-null values requires for a row or column to be retained\n",
    "\n",
    "# subset parameter\n",
    "df_spark.na.drop(subset = ['Product']) # drop rows or columns with missing values in the specified columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the missing values\n",
    "df_spark.na.fill('Missing Value') # fill all missing values with the specified value\n",
    "df_spark.na.fill('Missing Value', subset = ['Product']).show() # fill missing values in the specified columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the missing values with the mean value\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.sql.functions import collect_list, avg\n",
    "\n",
    "df_spark = df_spark.withColumn(\"Boxes Shipped\", df_spark[\"Boxes Shipped\"].cast(\"double\"))\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols = ['Boxes Shipped'],\n",
    "    outputCols = [\"Boxes Shipped_imputed\"]\n",
    "    ).setStrategy(\"mean\")\n",
    "\n",
    "imputer.fit(df_spark).transform(df_spark).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casting the data type\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "df_spark = df_spark.withColumn(\"Amount\", regexp_replace(df_spark[\"Amount\"], \"[$,]\", \"\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataset\n",
    "df_spark.filter(\"Amount > 15000\")\n",
    "\n",
    "# Filter the dataset selecting the columns\n",
    "df_spark.filter(\"`Boxes Shipped` < 300\").select(['Sales Person', 'Country'])\n",
    "\n",
    "# Filter the dataset using multiple conditions\n",
    "df_spark.filter((df_spark['Amount'] > 15000) & \n",
    "                (df_spark['Country'] == 'UK')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GroupBy and Aggregate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GroupBy operation\n",
    "df_spark.groupBy('Sales Person').sum().select(['Sales Person', 'sum(Boxes Shipped)']).show()\n",
    "\n",
    "df_spark.groupBy('Country').mean().select(['Country', 'avg(Amount)']).show()\n",
    "\n",
    "df_spark.groupBy('Country').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark.agg({'Amount': 'sum'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PySpark ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = spark.read.csv(\"../data/Chocolate Sales.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "training = training.withColumn(\"Amount\", regexp_replace(training[\"Amount\"], \"[$,]\", \"\").cast(\"int\"))\n",
    "training = training.withColumn(\"Salary\", training['Amount'] * training['Boxes Shipped'] * 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "feature_assembler = VectorAssembler(\n",
    "    inputCols = ['Amount', 'Boxes Shipped'],\n",
    "    outputCol = 'Independent Features'\n",
    ")\n",
    "\n",
    "output = feature_assembler.transform(training)\n",
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalized_data = output.select('Independent Features', 'Salary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "train_data, test_data = finalized_data.randomSplit([0.75, 0.25])\n",
    "regressor = LinearRegression(\n",
    "    featuresCol='Independent Features',\n",
    "    labelCol='Salary'\n",
    ")\n",
    "\n",
    "regressor = regressor.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the coefficients\n",
    "regressor.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the intercept\n",
    "regressor.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "pred_results = regressor.evaluate(test_data)\n",
    "\n",
    "pred_results.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results.meanAbsoluteError, pred_results.meanSquaredError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
