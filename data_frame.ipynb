{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Chocolate Sales').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "df_spark = spark.read.option(\"header\", \"true\").csv(\"Chocolate Sales.csv\", inferSchema = True)\n",
    "\n",
    "# Another option to read dataset\n",
    "spark.read.csv(\"Chocolate Sales.csv\", header=True, inferSchema=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the schema\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the column names\n",
    "df_spark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first 10 rows\n",
    "df_spark.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the Product column\n",
    "df_spark.select('Product').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select multiple columns\n",
    "df_spark.select(['Product', 'Country', 'Date']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the types of the columns\n",
    "df_spark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the summary of the dataset\n",
    "df_spark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column\n",
    "df_spark = df_spark.withColumn('New Column', df_spark['Boxes Shipped'] * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns\n",
    "df_spark = df_spark.drop('New Column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns\n",
    "df_spark.withColumnRenamed('Sales Person', 'Salesperson').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the missing values\n",
    "\n",
    "# how parameter\n",
    "df_spark.na.drop(how = 'any') # drop column if any missing value - default\n",
    "df_spark.na.drop(how = 'all') # drop column if all missing value\n",
    "\n",
    "# threshold parameter\n",
    "df_spark.na.drop(thresh = 2) # specify the minimum number of non-null values requires for a row or column to be retained\n",
    "\n",
    "# subset parameter\n",
    "df_spark.na.drop(subset = ['Product']) # drop rows or columns with missing values in the specified columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the missing values\n",
    "df_spark.na.fill('Missing Value') # fill all missing values with the specified value\n",
    "df_spark.na.fill('Missing Value', subset = ['Product']).show() # fill missing values in the specified columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the missing values with the mean value\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.sql.functions import collect_list, avg\n",
    "\n",
    "df_spark = df_spark.withColumn(\"Boxes Shipped\", df_spark[\"Boxes Shipped\"].cast(\"double\"))\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols = ['Boxes Shipped'],\n",
    "    outputCols = [\"Boxes Shipped_imputed\"]\n",
    "    ).setStrategy(\"mean\")\n",
    "\n",
    "imputer.fit(df_spark).transform(df_spark).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
